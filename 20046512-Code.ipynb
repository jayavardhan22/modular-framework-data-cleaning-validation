{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_JlAhiK4U-jx"
      },
      "outputs": [],
      "source": [
        "# BLOCK 1: Imports\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import re\n",
        "from datetime import datetime\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization (for later / Streamlit placeholder)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Outlier / clustering models\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Train-test split for RF\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Typing\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "# Pandera schema validation with fallback\n",
        "import warnings\n",
        "try:\n",
        "    import pandera.pandas as pa\n",
        "    from pandera.pandas import Column, Check, DataFrameSchema\n",
        "    PANDERA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    try:\n",
        "        import pandera as pa\n",
        "        from pandera import Column, Check, DataFrameSchema\n",
        "        PANDERA_AVAILABLE = True\n",
        "    except ImportError as e:\n",
        "        PANDERA_AVAILABLE = False\n",
        "        PANDERA_ERROR = str(e)\n",
        "        class pa:\n",
        "            class DateTime: pass\n",
        "            class String: pass\n",
        "            class errors:\n",
        "                class SchemaWarning: pass\n",
        "                class SchemaErrors(Exception): pass\n",
        "        Column = None\n",
        "        Check = None\n",
        "        DataFrameSchema = None\n",
        "\n",
        "if PANDERA_AVAILABLE:\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandera\")\n",
        "    try:\n",
        "        warnings.filterwarnings(\"ignore\", category=pa.errors.SchemaWarning)\n",
        "    except AttributeError:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IR4B3H-LVJZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9173aace-87a4-4e26-c144-60c2a9cbcd76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Block2] Raw sample (first 5 rows):\n",
            "           0           1         2         3         4           5   \\\n",
            "0  CRASH DATE  CRASH TIME   BOROUGH  ZIP CODE  LATITUDE   LONGITUDE   \n",
            "1  09/11/2021        2:39       NaN       NaN       NaN         NaN   \n",
            "2  03/26/2022       11:45       NaN       NaN       NaN         NaN   \n",
            "3  11/01/2023        1:29  BROOKLYN     11230  40.62179  -73.970024   \n",
            "4  06/29/2022        6:55       NaN       NaN       NaN         NaN   \n",
            "\n",
            "                       6                        7                  8   \\\n",
            "0                LOCATION           ON STREET NAME  CROSS STREET NAME   \n",
            "1                     NaN    WHITESTONE EXPRESSWAY          20 AVENUE   \n",
            "2                     NaN  QUEENSBORO BRIDGE UPPER                NaN   \n",
            "3  (40.62179, -73.970024)            OCEAN PARKWAY           AVENUE K   \n",
            "4                     NaN       THROGS NECK BRIDGE                NaN   \n",
            "\n",
            "                9   ...                             19  \\\n",
            "0  OFF STREET NAME  ...  CONTRIBUTING FACTOR VEHICLE 2   \n",
            "1              NaN  ...                    Unspecified   \n",
            "2              NaN  ...                            NaN   \n",
            "3              NaN  ...                    Unspecified   \n",
            "4              NaN  ...                    Unspecified   \n",
            "\n",
            "                              20                             21  \\\n",
            "0  CONTRIBUTING FACTOR VEHICLE 3  CONTRIBUTING FACTOR VEHICLE 4   \n",
            "1                            NaN                            NaN   \n",
            "2                            NaN                            NaN   \n",
            "3                    Unspecified                            NaN   \n",
            "4                            NaN                            NaN   \n",
            "\n",
            "                              22            23                   24  \\\n",
            "0  CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  VEHICLE TYPE CODE 1   \n",
            "1                            NaN       4455765                Sedan   \n",
            "2                            NaN       4513547                Sedan   \n",
            "3                            NaN       4675373                Moped   \n",
            "4                            NaN       4541903                Sedan   \n",
            "\n",
            "                    25                   26                   27  \\\n",
            "0  VEHICLE TYPE CODE 2  VEHICLE TYPE CODE 3  VEHICLE TYPE CODE 4   \n",
            "1                Sedan                  NaN                  NaN   \n",
            "2                  NaN                  NaN                  NaN   \n",
            "3                Sedan                Sedan                  NaN   \n",
            "4        Pick-up Truck                  NaN                  NaN   \n",
            "\n",
            "                    28  \n",
            "0  VEHICLE TYPE CODE 5  \n",
            "1                  NaN  \n",
            "2                  NaN  \n",
            "3                  NaN  \n",
            "4                  NaN  \n",
            "\n",
            "[5 rows x 29 columns]\n",
            "\n",
            "[Block2] Raw shape (approx, using full file head only above).\n"
          ]
        }
      ],
      "source": [
        "# BLOCK 2: File path, drive mount, describe data\n",
        "\n",
        "def mount_drive_if_needed():\n",
        "    \"\"\"\n",
        "    Placeholder for Google Drive mount (for Colab).\n",
        "    In Colab: from google.colab import drive; drive.mount('/content/drive')\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "DATASET_PATH = \"/content/sample_data/dataset.csv\"  # File_Path\n",
        "\n",
        "\n",
        "def describe_raw_data(path: str) -> None:\n",
        "    \"\"\"\n",
        "    Print basic info about the raw CSV before heavy processing.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[Block2] dataset not found at: {path}\")\n",
        "        return\n",
        "\n",
        "    sample = pd.read_csv(path, nrows=5, low_memory=False, on_bad_lines=\"skip\", encoding=\"utf-8\", header=None)\n",
        "    print(\"[Block2] Raw sample (first 5 rows):\")\n",
        "    print(sample.head())\n",
        "    print(\"\\n[Block2] Raw shape (approx, using full file head only above).\")\n",
        "\n",
        "\n",
        "describe_raw_data(DATASET_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jL10YUDfVf7O"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 3: Expected columns, feature engineering, initial cleaning (FIXED)\n",
        "# =============================================================================\n",
        "\n",
        "GEOSPATIAL_COLS = ['LATITUDE', 'LONGITUDE']\n",
        "\n",
        "INJURY_COLS = [\n",
        "    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
        "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
        "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
        "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'\n",
        "]\n",
        "\n",
        "EXPECTED_COLUMN_NAMES = [\n",
        "    'CRASH DATE', 'CRASH TIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE',\n",
        "    'ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME',\n",
        "    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
        "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
        "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
        "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 5',\n",
        "    'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3',\n",
        "    'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5',\n",
        "    'COLLISION_ID'\n",
        "]\n",
        "\n",
        "\n",
        "INJURY_FEATURE_COLS = [\n",
        "    'NUMBER OF PEDESTRIANS INJURED',\n",
        "    'NUMBER OF PEDESTRIANS KILLED',\n",
        "    'NUMBER OF CYCLIST INJURED',\n",
        "    'NUMBER OF CYCLIST KILLED',\n",
        "    'NUMBER OF MOTORIST INJURED',\n",
        "    'NUMBER OF MOTORIST KILLED'\n",
        "]\n",
        "\n",
        "def load_and_clean_data(file_name: str) -> Tuple[pd.DataFrame, int]:\n",
        "    \"\"\"\n",
        "    Loads CSV, applies expected column headers, performs basic type cleaning,\n",
        "    creates CRASH DATETIME and TOTAL_CASUALTIES, and computes LOC (lines of file).\n",
        "    Returns cleaned df and LOC.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_name):\n",
        "        raise FileNotFoundError(f\"[Block3] File not found: {file_name}\")\n",
        "\n",
        "    # LOC (Lines Of Code/Lines Of CSV)\n",
        "    with open(file_name, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        loc = sum(1 for _ in f)\n",
        "\n",
        "    df = pd.read_csv(\n",
        "        file_name,\n",
        "        encoding=\"utf-8\",\n",
        "        header=None,\n",
        "        engine=\"python\",    # <-- IMPORTANT\n",
        "        on_bad_lines=\"skip\"  # <-- skips the broken row\n",
        "    )\n",
        "\n",
        "    # Align columns\n",
        "    if len(df.columns) != len(EXPECTED_COLUMN_NAMES):\n",
        "        if len(df.columns) < len(EXPECTED_COLUMN_NAMES):\n",
        "            df.columns = EXPECTED_COLUMN_NAMES[:len(df.columns)]\n",
        "        else:\n",
        "            df = df.iloc[:, :len(EXPECTED_COLUMN_NAMES)]\n",
        "            df.columns = EXPECTED_COLUMN_NAMES\n",
        "    else:\n",
        "        df.columns = EXPECTED_COLUMN_NAMES\n",
        "\n",
        "    # Date / time cleaning and merge\n",
        "    df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'], format='%m/%d/%Y', errors='coerce')\n",
        "    df['CRASH TIME'] = df['CRASH TIME'].astype(str).str.strip().str.slice(0, 5)\n",
        "    df['CRASH DATETIME'] = pd.to_datetime(\n",
        "        df['CRASH DATE'].dt.strftime('%Y-%m-%d') + ' ' + df['CRASH TIME'],\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "    # Injury columns to numeric\n",
        "    for col in INJURY_COLS:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(np.int32)\n",
        "\n",
        "    # To be extra explicit\n",
        "    for col in INJURY_FEATURE_COLS:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(np.int32)\n",
        "\n",
        "    # Feature used later: TOTAL_CASUALTIES\n",
        "    df['TOTAL_CASUALTIES'] = df[INJURY_COLS].sum(axis=1).astype(np.int32)\n",
        "\n",
        "    return df.reset_index(drop=True), loc\n",
        "\n",
        "df, loc = load_and_clean_data(DATASET_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BLOCK 3A: Output Summary (Expected Columns & Feature Engineering)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[Block3] Feature Engineering Summary\")\n",
        "print(\"-------------------------------------------------------\")\n",
        "\n",
        "print(\"[Block3] Total rows loaded        :\", f\"{len(df):,}\")\n",
        "print(\"[Block3] Columns aligned          :\", f\"{len(df.columns)} / {len(EXPECTED_COLUMN_NAMES)}\")\n",
        "print(\"[Block3] CRASH DATETIME created\")\n",
        "print(\"[Block3] Injury columns coerced to numeric\")\n",
        "print(\"[Block3] Feature engineered       : TOTAL_CASUALTIES\")\n",
        "\n",
        "print(\"[Block3] Type coercion summary:\")\n",
        "print(f\"- Dates coerced   : {df['CRASH DATETIME'].isna().sum()} failures\")\n",
        "print(\"- Numeric fixes   : applied to injury columns\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMxPeHeKYOO4",
        "outputId": "409d76f3-bebc-44d2-ef77-593648ec0950"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Block3] Feature Engineering Summary\n",
            "-------------------------------------------------------\n",
            "[Block3] Total rows loaded        : 116,632\n",
            "[Block3] Columns aligned          : 30 / 28\n",
            "[Block3] CRASH DATETIME created\n",
            "[Block3] Injury columns coerced to numeric\n",
            "[Block3] Feature engineered       : TOTAL_CASUALTIES\n",
            "[Block3] Type coercion summary:\n",
            "- Dates coerced   : 1 failures\n",
            "- Numeric fixes   : applied to injury columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EhLaxXPWdOI",
        "outputId": "4b528298-5e62-4cf8-f1f8-aff16bfaa9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n-rhB_yAizj",
        "outputId": "8ec6c33e-be5a-4cbe-db6d-1ca9ccb1762a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJKbotx6Via6",
        "outputId": "e53ab1c3-a765-4a83-cb4a-af429a59db8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Block4] Validation summary: {'Rule_Violation_Geospatial': '1908 records outside NYC bounds.', 'Rule_Violation_Logic_Injury_Count': '302 records where total injured < sum of pedestrian/cyclist/motorist.'}\n",
            "[Block4] Total violations: 2210 Time: 0.1161s\n"
          ]
        }
      ],
      "source": [
        "# BLOCK 4: Schema validation and rule-based checks\n",
        "\n",
        "if PANDERA_AVAILABLE:\n",
        "    try:\n",
        "        from pandera import dtypes\n",
        "        core_schema = pa.DataFrameSchema({\n",
        "            'COLLISION_ID': Column(dtypes.Int64, nullable=False, checks=Check.greater_than(0)),\n",
        "            'CRASH DATE': Column(dtypes.DateTime, nullable=False),\n",
        "            'CRASH TIME': Column(dtypes.String, nullable=False, checks=Check.str_matches(r'^\\d{2}:\\d{2}$')),\n",
        "            'NUMBER OF PERSONS INJURED': Column(dtypes.Int32, checks=[Check.greater_than_or_equal_to(0)]),\n",
        "            'LATITUDE': Column(dtypes.Float64, nullable=True),\n",
        "            'LONGITUDE': Column(dtypes.Float64, nullable=True),\n",
        "        }, strict=False)\n",
        "    except (ImportError, AttributeError):\n",
        "        core_schema = None\n",
        "else:\n",
        "    core_schema = None\n",
        "\n",
        "\n",
        "def schema_and_rule_validation(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any], int, float]:\n",
        "    \"\"\"\n",
        "    Apply Pandera schema and rule-based checks (geospatial + injury logic).\n",
        "    Returns: validated df, validation_report, total_violations, execution_time.\n",
        "    \"\"\"\n",
        "    validation_report: Dict[str, Any] = {}\n",
        "    total_violations = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Pandera schema checks\n",
        "    if core_schema is not None and PANDERA_AVAILABLE:\n",
        "        try:\n",
        "            df = core_schema.validate(df, lazy=True)\n",
        "        except pa.errors.SchemaErrors as err:\n",
        "            for column, errors in err.failure_cases.groupby('column'):\n",
        "                validation_report[f\"Schema_Violation_Pandera_{column}\"] = f\"{len(errors)} errors (check/type/missing).\"\n",
        "                total_violations += len(errors)\n",
        "\n",
        "    # Ensure numeric lat/lon\n",
        "    df['LATITUDE'] = pd.to_numeric(df['LATITUDE'], errors='coerce')\n",
        "    df['LONGITUDE'] = pd.to_numeric(df['LONGITUDE'], errors='coerce')\n",
        "\n",
        "    # Geospatial rule: NYC-ish bounds\n",
        "    lat_violations = df[(df['LATITUDE'] < 40.4) | (df['LATITUDE'] > 41.0)].dropna(subset=['LATITUDE']).shape[0]\n",
        "    lon_violations = df[(df['LONGITUDE'] < -74.3) | (df['LONGITUDE'] > -73.7)].dropna(subset=['LONGITUDE']).shape[0]\n",
        "    if lat_violations + lon_violations > 0:\n",
        "        validation_report[\"Rule_Violation_Geospatial\"] = f\"{lat_violations + lon_violations} records outside NYC bounds.\"\n",
        "        total_violations += (lat_violations + lon_violations)\n",
        "\n",
        "    # Logical rule: total injured >= sum of parts\n",
        "    total_injured = df['NUMBER OF PERSONS INJURED']\n",
        "    parts_injured = (\n",
        "        df['NUMBER OF PEDESTRIANS INJURED'] +\n",
        "        df['NUMBER OF CYCLIST INJURED'] +\n",
        "        df['NUMBER OF MOTORIST INJURED']\n",
        "    )\n",
        "    inconsistency_count = (total_injured < parts_injured).sum()\n",
        "    if inconsistency_count > 0:\n",
        "        validation_report[\"Rule_Violation_Logic_Injury_Count\"] = (\n",
        "            f\"{inconsistency_count} records where total injured < sum of pedestrian/cyclist/motorist.\"\n",
        "        )\n",
        "        total_violations += inconsistency_count\n",
        "\n",
        "    exec_time = time.time() - start_time\n",
        "    print(\"[Block4] Validation summary:\", validation_report)\n",
        "    print(\"[Block4] Total violations:\", total_violations, \"Time:\", f\"{exec_time:.4f}s\")\n",
        "\n",
        "    return df, validation_report, total_violations, exec_time\n",
        "\n",
        "validated_df, validation_summary, total_violations, exec_time = schema_and_rule_validation(df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BLOCK 4A: Output Summary (Schema & Rule Validation)\n",
        "# =============================================================================\n",
        "\n",
        "import re\n",
        "\n",
        "def _extract_count(val):\n",
        "    \"\"\"Extract numeric count safely from validation summary values\"\"\"\n",
        "    if isinstance(val, str):\n",
        "        m = re.search(r'\\d+', val)\n",
        "        return int(m.group()) if m else 0\n",
        "    if isinstance(val, (int, float)):\n",
        "        return int(val)\n",
        "    return 0\n",
        "\n",
        "print(\"\\n[Block4] Schema validation started\")\n",
        "print(\"[Block4] Pandera schema applied (strict=False)\\n\")\n",
        "\n",
        "print(\"[Block4] Rule Violations Detected:\")\n",
        "\n",
        "schema_v = sum(\n",
        "    _extract_count(v)\n",
        "    for k, v in validation_summary.items()\n",
        "    if \"Schema\" in k\n",
        ")\n",
        "\n",
        "geo_v = sum(\n",
        "    _extract_count(v)\n",
        "    for k, v in validation_summary.items()\n",
        "    if \"Geospatial\" in k\n",
        ")\n",
        "\n",
        "logic_v = sum(\n",
        "    _extract_count(v)\n",
        "    for k, v in validation_summary.items()\n",
        "    if \"Logic\" in k\n",
        ")\n",
        "\n",
        "print(f\"- Schema violations          : {schema_v:,}\")\n",
        "print(f\"- Geospatial bounds issues   : {geo_v:,}\")\n",
        "print(f\"- Injury logic inconsistencies: {logic_v:,}\")\n",
        "\n",
        "print(f\"\\n[Block4] Total violations    : {total_violations:,}\")\n",
        "print(f\"[Block4] Execution time      : {exec_time:.4f}s\")\n",
        "\n",
        "\n",
        "# =============================\n",
        "# BLOCK 4A: Rule-based labels for ML (y_true)\n",
        "# =============================\n",
        "# y_true is derived from rule-based validation results\n",
        "# This will be used by Random Forest in Block 6\n",
        "\n",
        "# Initialize all as normal (0)\n",
        "y_true = np.zeros(len(df), dtype=int)\n",
        "\n",
        "# If rule-based violations exist, mark as anomaly (1)\n",
        "# Here we use TOTAL_VIOLATIONS logic from Block 4\n",
        "if \"total_violations\" in locals() and total_violations > 0:\n",
        "    # Mark rows violating injury logic as anomalies\n",
        "    if \"NUMBER OF PERSONS INJURED\" in df.columns and \"TOTAL_CASUALTIES\" in df.columns:\n",
        "        injury_logic_mask = df[\"TOTAL_CASUALTIES\"] < df[INJURY_COLS].sum(axis=1)\n",
        "        y_true[injury_logic_mask.values] = 1\n",
        "\n",
        "# Safety fallback: if no violations detected, keep y_true all zeros\n",
        "print(\"[Block4A] y_true labels created\")\n",
        "print(f\"[Block4A] Total samples        : {len(y_true):,}\")\n",
        "print(f\"[Block4A] Anomalous labels     : {int(y_true.sum()):,}\")\n",
        "print(f\"[Block4A] Normal labels        : {len(y_true) - int(y_true.sum()):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_K5l_qYs-0",
        "outputId": "dea34d21-f32d-4655-c125-48913f7cd57f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Block4] Schema validation started\n",
            "[Block4] Pandera schema applied (strict=False)\n",
            "\n",
            "[Block4] Rule Violations Detected:\n",
            "- Schema violations          : 0\n",
            "- Geospatial bounds issues   : 1,908\n",
            "- Injury logic inconsistencies: 302\n",
            "\n",
            "[Block4] Total violations    : 2,210\n",
            "[Block4] Execution time      : 0.1161s\n",
            "[Block4A] y_true labels created\n",
            "[Block4A] Total samples        : 116,632\n",
            "[Block4A] Anomalous labels     : 0\n",
            "[Block4A] Normal labels        : 116,632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_K_4kabVlm6",
        "outputId": "40023eb2-99bc-4f2e-cb69-64f1c7648ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Block5] IF anomalies: 1167 | DBSCAN sampled anomalies: 67 | LOF anomalies: 1073 | LOC anomalies: 1166\n",
            "[Block5] Baseline violations: 7 Time: 0.1071s\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 5: Isolation Forest, DBSCAN, LOF (Local Outlier Factor), baseline comparison (FIXED)\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.cluster import KMeans  # needed for LOC (kmeans-distance) method\n",
        "\n",
        "def anomaly_detection(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Isolation Forest + DBSCAN + LOF using:\n",
        "    Features = [LAT, LON] + individual injury columns (plus TOTAL_CASUALTIES already exists).\n",
        "    Also includes your LOC-style clustering using KMeans distance (kept as ANOMALY_FLAG_LOC).\n",
        "\n",
        "    Output flags:\n",
        "      - ANOMALY_FLAG_IF     : -1 anomaly,  1 normal\n",
        "      - ANOMALY_FLAG_DBSCAN : -1 anomaly/noise in sample, other labels otherwise, 0 for non-sampled rows\n",
        "      - ANOMALY_FLAG_LOF    : -1 anomaly,  1 normal (only computed where coords exist; others set to 1)\n",
        "      - ANOMALY_FLAG_LOC    :  1 outlier,  0 normal  (KMeans-distance heuristic)\n",
        "    \"\"\"\n",
        "    performance_metrics: Dict[str, float] = {}\n",
        "\n",
        "    # --- Feature set (Req #1: separate injury columns) ---\n",
        "    model_features = GEOSPATIAL_COLS + INJURY_COLS\n",
        "    present_features = [c for c in model_features if c in df.columns]\n",
        "\n",
        "    # Make sure required cols exist\n",
        "    if 'LATITUDE' not in present_features or 'LONGITUDE' not in present_features:\n",
        "        raise ValueError(\"[Block5] LATITUDE/LONGITUDE missing from dataframe.\")\n",
        "\n",
        "    # Build matrix\n",
        "    anomaly_data = df[present_features].copy()\n",
        "\n",
        "    # Fill numeric missing values with medians\n",
        "    for c in present_features:\n",
        "        anomaly_data[c] = pd.to_numeric(anomaly_data[c], errors='coerce')\n",
        "        if anomaly_data[c].isna().any():\n",
        "            anomaly_data[c] = anomaly_data[c].fillna(anomaly_data[c].median())\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(anomaly_data)\n",
        "\n",
        "    del anomaly_data\n",
        "    gc.collect()\n",
        "\n",
        "    dataset_size = len(df)\n",
        "    if dataset_size > 1_000_000:\n",
        "        max_samples = min(100_000, dataset_size)\n",
        "    elif dataset_size > 500_000:\n",
        "        max_samples = min(50_000, dataset_size)\n",
        "    else:\n",
        "        max_samples = \"auto\"\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Isolation Forest\n",
        "    # -------------------------------------------------------------------------\n",
        "    start_time_if = time.time()\n",
        "    iso_forest = IsolationForest(\n",
        "        contamination=0.01,\n",
        "        random_state=42,\n",
        "        n_jobs=2,\n",
        "        max_samples=max_samples,\n",
        "        verbose=0\n",
        "    )\n",
        "    df[\"ANOMALY_FLAG_IF\"] = iso_forest.fit_predict(scaled_data)\n",
        "    anomalies_if_count = int((df[\"ANOMALY_FLAG_IF\"] == -1).sum())\n",
        "    time_if = time.time() - start_time_if\n",
        "\n",
        "    performance_metrics[\"IF_Time_s\"] = time_if\n",
        "    performance_metrics[\"IF_Anomaly_Count\"] = anomalies_if_count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # DBSCAN (sampled)\n",
        "    # -------------------------------------------------------------------------\n",
        "    if dataset_size > 1_000_000:\n",
        "        dbscan_sample_size = 10_000\n",
        "    elif dataset_size > 500_000:\n",
        "        dbscan_sample_size = 20_000\n",
        "    elif dataset_size > 100_000:\n",
        "        dbscan_sample_size = 30_000\n",
        "    else:\n",
        "        dbscan_sample_size = min(50_000, dataset_size)\n",
        "\n",
        "    dbscan_sample_size = min(dbscan_sample_size, len(scaled_data))\n",
        "    dbs_sample = scaled_data[:dbscan_sample_size]\n",
        "\n",
        "    start_time_dbs = time.time()\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5, n_jobs=2)\n",
        "    dbscan_labels = dbscan.fit_predict(dbs_sample)\n",
        "    anomalies_dbs_count = int((dbscan_labels == -1).sum())\n",
        "    time_dbs = time.time() - start_time_dbs\n",
        "\n",
        "    df[\"ANOMALY_FLAG_DBSCAN\"] = 0\n",
        "    df.loc[:len(dbscan_labels) - 1, \"ANOMALY_FLAG_DBSCAN\"] = dbscan_labels\n",
        "\n",
        "    performance_metrics[\"DBSCAN_Time_s\"] = time_dbs\n",
        "    performance_metrics[\"DBSCAN_Anomaly_Count_Sample\"] = anomalies_dbs_count\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # LOF (Req #2) â€” run on LAT/LON ONLY to handle varying density (Manhattan vs SI)\n",
        "    # fit_predict returns 1 for inliers and -1 for outliers. [web:56][web:58]\n",
        "    # -------------------------------------------------------------------------\n",
        "    start_time_lof = time.time()\n",
        "\n",
        "    lof_mask = df[\"LATITUDE\"].notna() & df[\"LONGITUDE\"].notna()\n",
        "    lof_features = df.loc[lof_mask, [\"LATITUDE\", \"LONGITUDE\"]].copy()\n",
        "\n",
        "    # Fill any leftover numeric NAs (safety)\n",
        "    lof_features[\"LATITUDE\"] = pd.to_numeric(lof_features[\"LATITUDE\"], errors=\"coerce\")\n",
        "    lof_features[\"LONGITUDE\"] = pd.to_numeric(lof_features[\"LONGITUDE\"], errors=\"coerce\")\n",
        "    lof_features = lof_features.dropna()\n",
        "\n",
        "    df[\"ANOMALY_FLAG_LOF\"] = 1  # default inlier\n",
        "    anomalies_lof_count = 0\n",
        "\n",
        "    if len(lof_features) >= 50:\n",
        "        lof_scaler = StandardScaler()\n",
        "        lof_scaled = lof_scaler.fit_transform(lof_features)\n",
        "\n",
        "        lof = LocalOutlierFactor(\n",
        "            n_neighbors=20,\n",
        "            contamination=0.01,\n",
        "            n_jobs=2\n",
        "        )\n",
        "        lof_labels = lof.fit_predict(lof_scaled)  # 1=inlier, -1=outlier\n",
        "        df.loc[lof_features.index, \"ANOMALY_FLAG_LOF\"] = lof_labels\n",
        "        anomalies_lof_count = int((lof_labels == -1).sum())\n",
        "    else:\n",
        "        # Not enough points to run LOF reliably\n",
        "        anomalies_lof_count = 0\n",
        "\n",
        "    time_lof = time.time() - start_time_lof\n",
        "    performance_metrics[\"LOF_Time_s\"] = time_lof\n",
        "    performance_metrics[\"LOF_Anomaly_Count\"] = anomalies_lof_count\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # LOC : KMeans distance outliers\n",
        "    # -------------------------------------------------------------------------\n",
        "    start_time_loc = time.time()\n",
        "\n",
        "    kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "    kmeans_labels = kmeans.fit_predict(scaled_data)\n",
        "    centers = kmeans.cluster_centers_\n",
        "\n",
        "    dists = np.linalg.norm(scaled_data - centers[kmeans_labels], axis=1)\n",
        "    threshold = np.quantile(dists, 0.99)\n",
        "\n",
        "    df[\"ANOMALY_FLAG_LOC\"] = (dists > threshold).astype(int)  # 1 = outlier\n",
        "    anomalies_loc_count = int((df[\"ANOMALY_FLAG_LOC\"] == 1).sum())\n",
        "\n",
        "    time_loc = time.time() - start_time_loc\n",
        "    performance_metrics[\"LOC_Time_s\"] = time_loc\n",
        "    performance_metrics[\"LOC_Anomaly_Count\"] = anomalies_loc_count\n",
        "\n",
        "    print(\n",
        "        \"[Block5] IF anomalies:\", anomalies_if_count,\n",
        "        \"| DBSCAN sampled anomalies:\", anomalies_dbs_count,\n",
        "        \"| LOF anomalies:\", anomalies_lof_count,\n",
        "        \"| LOC anomalies:\", anomalies_loc_count\n",
        "    )\n",
        "\n",
        "    del iso_forest, dbscan, kmeans, dbs_sample, scaled_data, dbscan_labels, centers, dists\n",
        "    gc.collect()\n",
        "\n",
        "    return df, performance_metrics\n",
        "\n",
        "\n",
        "def run_baseline_comparison(df: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Baseline: simple Pandas rule check (e.g., NUMBER OF PERSONS KILLED > 10).\n",
        "    \"\"\"\n",
        "    baseline_metrics: Dict[str, float] = {}\n",
        "    start_time_base = time.time()\n",
        "\n",
        "    baseline_df = df.copy()\n",
        "    for col in INJURY_COLS:\n",
        "        if col in baseline_df.columns:\n",
        "            baseline_df[col] = pd.to_numeric(baseline_df[col], errors=\"coerce\").fillna(0)\n",
        "\n",
        "    violation_col = \"NUMBER OF PERSONS KILLED\"\n",
        "    if violation_col in baseline_df.columns:\n",
        "        baseline_violations_count = int((baseline_df[violation_col] > 10).sum())\n",
        "    else:\n",
        "        baseline_violations_count = 0\n",
        "\n",
        "    time_base = time.time() - start_time_base\n",
        "    baseline_metrics[\"Baseline_Time_s\"] = time_base\n",
        "    baseline_metrics[\"Baseline_Violations\"] = baseline_violations_count\n",
        "\n",
        "    print(\"[Block5] Baseline violations:\", baseline_violations_count, \"Time:\", f\"{time_base:.4f}s\")\n",
        "\n",
        "    del baseline_df\n",
        "    gc.collect()\n",
        "    return baseline_metrics\n",
        "\n",
        "\n",
        "\n",
        "# =============================\n",
        "# RUN BLOCK 5 (COMPUTATION)\n",
        "# =============================\n",
        "\n",
        "df, block5_metrics = anomaly_detection(df)\n",
        "baseline_metrics = run_baseline_comparison(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BLOCK 5A: Output Summary (Reporting Only)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[Block5] Feature set:\")\n",
        "print(\"- Geospatial (LATITUDE, LONGITUDE)\")\n",
        "print(\"- Individual injury columns\")\n",
        "\n",
        "print(\"\\n[Block5] Isolation Forest:\")\n",
        "print(f\"- Anomalies detected : {block5_metrics['IF_Anomaly_Count']:,}\")\n",
        "print(f\"- Time               : {block5_metrics['IF_Time_s']:.2f}s\")\n",
        "\n",
        "print(\"\\n[Block5] DBSCAN (sampled):\")\n",
        "print(f\"- Noise points       : {block5_metrics['DBSCAN_Anomaly_Count_Sample']:,}\")\n",
        "print(f\"- Time               : {block5_metrics['DBSCAN_Time_s']:.2f}s\")\n",
        "\n",
        "print(\"\\n[Block5] LOF:\")\n",
        "print(f\"- Density anomalies  : {block5_metrics['LOF_Anomaly_Count']:,}\")\n",
        "print(f\"- Time               : {block5_metrics['LOF_Time_s']:.2f}s\")\n",
        "\n",
        "print(\"\\n[Block5] LOC (KMeans-distance):\")\n",
        "print(f\"- Outliers detected  : {block5_metrics['LOC_Anomaly_Count']:,}\")\n",
        "print(f\"- Time               : {block5_metrics['LOC_Time_s']:.2f}s\")\n",
        "\n",
        "print(\"\\n[Block5] Baseline rule (PERSONS_KILLED > 10):\")\n",
        "print(f\"- Violations found   : {baseline_metrics['Baseline_Violations']:,}\")\n",
        "print(f\"- Time               : {baseline_metrics['Baseline_Time_s']:.4f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBZdvJIvbcue",
        "outputId": "ce934ff3-7403-44f0-bbee-f3b7cc491fba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Block5] Feature set:\n",
            "- Geospatial (LATITUDE, LONGITUDE)\n",
            "- Individual injury columns\n",
            "\n",
            "[Block5] Isolation Forest:\n",
            "- Anomalies detected : 1,167\n",
            "- Time               : 1.77s\n",
            "\n",
            "[Block5] DBSCAN (sampled):\n",
            "- Noise points       : 67\n",
            "- Time               : 29.44s\n",
            "\n",
            "[Block5] LOF:\n",
            "- Density anomalies  : 1,073\n",
            "- Time               : 1.76s\n",
            "\n",
            "[Block5] LOC (KMeans-distance):\n",
            "- Outliers detected  : 1,166\n",
            "- Time               : 2.20s\n",
            "\n",
            "[Block5] Baseline rule (PERSONS_KILLED > 10):\n",
            "- Violations found   : 7\n",
            "- Time               : 0.1071s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0azsDppnVpHU"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 6: Random Forest\n",
        "#   (A) Supervised baseline on rule labels\n",
        "#   (B) Borough imputation\n",
        "# =============================================================================\n",
        "\n",
        "def train_random_forest(df: pd.DataFrame, y_true: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train RandomForestClassifier on rule-based labels.\n",
        "    Features: LATITUDE, LONGITUDE, TOTAL_CASUALTIES\n",
        "    \"\"\"\n",
        "\n",
        "    if y_true is None or len(y_true) != len(df):\n",
        "        raise ValueError(\"[Block6] y_true not initialized or length mismatch\")\n",
        "\n",
        "    feature_cols = GEOSPATIAL_COLS + [\"TOTAL_CASUALTIES\"]\n",
        "    X_rf = df[feature_cols].copy()\n",
        "\n",
        "    X_rf = X_rf.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    X_rf = X_rf.fillna(X_rf.median(numeric_only=True))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_rf, y_true, test_size=0.3, random_state=42, stratify=y_true\n",
        "    )\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    start_t = time.time()\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_rf)\n",
        "    exec_time = time.time() - start_t\n",
        "\n",
        "    metrics = {\n",
        "        \"training_rows\": int(len(X_train)),\n",
        "        \"feature_count\": int(X_train.shape[1]),\n",
        "        \"train_accuracy\": float(rf.score(X_train, y_train)),\n",
        "        \"time_s\": float(exec_time)\n",
        "    }\n",
        "\n",
        "    return y_pred, metrics\n",
        "\n",
        "\n",
        "def random_forest_borough_imputation(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Borough imputation using Random Forest\n",
        "    \"\"\"\n",
        "\n",
        "    stats = {\n",
        "        \"missing_before\": 0,\n",
        "        \"train_rows\": 0,\n",
        "        \"imputed\": 0,\n",
        "        \"train_accuracy\": None,\n",
        "        \"time_s\": 0.0\n",
        "    }\n",
        "\n",
        "    coords_mask = df[\"LATITUDE\"].notna() & df[\"LONGITUDE\"].notna()\n",
        "    train_mask = coords_mask & df[\"BOROUGH\"].notna()\n",
        "    predict_mask = coords_mask & df[\"BOROUGH\"].isna()\n",
        "\n",
        "    stats[\"missing_before\"] = int(predict_mask.sum())\n",
        "    stats[\"train_rows\"] = int(train_mask.sum())\n",
        "\n",
        "    if stats[\"train_rows\"] == 0 or stats[\"missing_before\"] == 0:\n",
        "        print(\"[Block6] Borough imputation skipped (no missing values).\")\n",
        "        return df, stats\n",
        "\n",
        "    X_train = df.loc[train_mask, [\"LATITUDE\", \"LONGITUDE\"]]\n",
        "    y_train = df.loc[train_mask, \"BOROUGH\"]\n",
        "    X_pred = df.loc[predict_mask, [\"LATITUDE\", \"LONGITUDE\"]]\n",
        "\n",
        "    X_train = X_train.apply(pd.to_numeric, errors=\"coerce\").fillna(X_train.median())\n",
        "    X_pred = X_pred.apply(pd.to_numeric, errors=\"coerce\").fillna(X_train.median())\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    start_t = time.time()\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    stats[\"train_accuracy\"] = float(rf.score(X_train, y_train))\n",
        "    df.loc[predict_mask, \"BOROUGH\"] = rf.predict(X_pred)\n",
        "\n",
        "    stats[\"imputed\"] = int(predict_mask.sum())\n",
        "    stats[\"time_s\"] = float(time.time() - start_t)\n",
        "\n",
        "    df[\"BOROUGH_IMPUTED\"] = 0\n",
        "    df.loc[predict_mask, \"BOROUGH_IMPUTED\"] = 1\n",
        "\n",
        "    return df, stats\n",
        "\n",
        "\n",
        "# =============================\n",
        "# RUN BLOCK 6 (COMPUTATION)\n",
        "# =============================\n",
        "\n",
        "y_pred_rf, rf_metrics = train_random_forest(df, y_true)\n",
        "df, rf_borough_stats = random_forest_borough_imputation(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BLOCK 6A: Random Forest Output\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[Block6] Random Forest (Rule-supervised)\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(f\"- Training rows     : {rf_metrics['training_rows']:,}\")\n",
        "print(f\"- Features          : LATITUDE, LONGITUDE, TOTAL_CASUALTIES\")\n",
        "print(f\"- Training accuracy : {rf_metrics['train_accuracy']:.4f}\")\n",
        "print(f\"- Training + infer  : {rf_metrics['time_s']:.2f}s\")\n",
        "\n",
        "print(\"\\n[Block6] Borough Imputation (RF)\")\n",
        "print(f\"- Missing before    : {rf_borough_stats['missing_before']:,}\")\n",
        "print(f\"- Training rows     : {rf_borough_stats['train_rows']:,}\")\n",
        "print(f\"- Imputed records   : {rf_borough_stats['imputed']:,}\")\n",
        "print(f\"- Training accuracy : {rf_borough_stats['train_accuracy']:.4f}\")\n",
        "print(f\"- Time              : {rf_borough_stats['time_s']:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE-V3IuWohnq",
        "outputId": "a0590ab1-9043-4d5d-b856-c82764f564fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Block6] Random Forest (Rule-supervised)\n",
            "--------------------------------------------------------\n",
            "- Training rows     : 81,642\n",
            "- Features          : LATITUDE, LONGITUDE, TOTAL_CASUALTIES\n",
            "- Training accuracy : 1.0000\n",
            "- Training + infer  : 0.79s\n",
            "\n",
            "[Block6] Borough Imputation (RF)\n",
            "- Missing before    : 32,833\n",
            "- Training rows     : 74,438\n",
            "- Imputed records   : 32,833\n",
            "- Training accuracy : 0.9936\n",
            "- Time              : 16.30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "g9xsCi01VwS0"
      },
      "outputs": [],
      "source": [
        "# BLOCK 7: Metrics and summary for all models\n",
        "\n",
        "def compute_metrics_and_summarize(\n",
        "    df: pd.DataFrame,\n",
        "    validation_report: Dict[str, Any],\n",
        "    performance_metrics: Dict[str, float],\n",
        "    baseline_metrics: Dict[str, float],\n",
        "    total_violations_before_correction: int,\n",
        "    time_module2_execution: float,\n",
        "    y_rf_pred: np.ndarray,\n",
        "    rf_time: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build ground truth from rule violations, then compare IF, DBSCAN, LOC, RF.\n",
        "    Returns a summary DataFrame with Precision, Recall, F1, EDR and timings.\n",
        "    \"\"\"\n",
        "    # Ground-truth label = rule violation\n",
        "    is_rule_violation = np.zeros(len(df), dtype=int)\n",
        "\n",
        "    total_injured = df['NUMBER OF PERSONS INJURED']\n",
        "    parts_injured = (\n",
        "        df['NUMBER OF PEDESTRIANS INJURED'] +\n",
        "        df['NUMBER OF CYCLIST INJURED'] +\n",
        "        df['NUMBER OF MOTORIST INJURED']\n",
        "    )\n",
        "    is_rule_violation[(total_injured < parts_injured).values] = 1\n",
        "\n",
        "    is_geospatial_violation = (\n",
        "        (df['LATITUDE'] < 40.4) | (df['LATITUDE'] > 41.0) |\n",
        "        (df['LONGITUDE'] < -74.3) | (df['LONGITUDE'] > -73.7)\n",
        "    ).fillna(False).values\n",
        "    is_rule_violation[is_geospatial_violation] = 1\n",
        "\n",
        "    y_true = is_rule_violation\n",
        "    results: List[List[Any]] = []\n",
        "\n",
        "    # Model outputs:\n",
        "    anomaly_models: Dict[str, Tuple[np.ndarray, float]] = {\n",
        "        \"Isolation Forest\": (\n",
        "            df['ANOMALY_FLAG_IF'].map({-1: 1, 1: 0}).values,\n",
        "            performance_metrics['IF_Time_s']\n",
        "        ),\n",
        "        \"DBSCAN (Sample)\": (\n",
        "            (df['ANOMALY_FLAG_DBSCAN'] == -1).astype(int).values[:len(y_true)],\n",
        "            performance_metrics['DBSCAN_Time_s']\n",
        "        ),\n",
        "        \"LOC (KMeans Distance)\": (\n",
        "            df['ANOMALY_FLAG_LOC'].astype(int).values,\n",
        "            performance_metrics['LOC_Time_s']\n",
        "        ),\n",
        "        \"Random Forest (Supervised)\": (\n",
        "            y_rf_pred,\n",
        "            rf_time\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Compute metrics\n",
        "    for name, (y_pred, exec_time) in anomaly_models.items():\n",
        "        if y_pred is None or exec_time is None:\n",
        "            continue\n",
        "\n",
        "        if \"DBSCAN\" in name:\n",
        "            y_true_temp = y_true[:len(y_pred)]\n",
        "        else:\n",
        "            y_true_temp = y_true\n",
        "\n",
        "        prec = precision_score(y_true_temp, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_true_temp, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true_temp, y_pred, zero_division=0)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true_temp, y_pred, labels=[0, 1]).ravel()\n",
        "        edr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "        results.append([\n",
        "            name,\n",
        "            f\"{exec_time:.4f}s\",\n",
        "            f\"{prec:.4f}\",\n",
        "            f\"{rec:.4f}\",\n",
        "            f\"{f1:.4f}\",\n",
        "            f\"{edr:.4f}\"\n",
        "        ])\n",
        "\n",
        "    # Rule-based \"oracle\"\n",
        "    results.append([\n",
        "        \"Rule-Based Validation (Pandera + Logic)\",\n",
        "        f\"{time_module2_execution:.4f}s\",\n",
        "        \"N/A\", \"1.0000\", \"N/A\", \"1.0000\"\n",
        "    ])\n",
        "\n",
        "    # Baseline\n",
        "    results.append([\n",
        "        \"Baseline (Pandas Proxy)\",\n",
        "        f\"{baseline_metrics['Baseline_Time_s']:.4f}s\",\n",
        "        \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
        "    ])\n",
        "\n",
        "    cols = [\"Module/Model\", \"Execution Time (Speed)\", \"Precision\", \"Recall\", \"F1-Score\", \"EDR\"]\n",
        "    results_df = pd.DataFrame(results, columns=cols).sort_values(\n",
        "        by=\"F1-Score\", ascending=False, na_position=\"last\"\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    print(\"[Block7] Metrics summary:\")\n",
        "    print(results_df)\n",
        "\n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BLOCK 7A: Model Comparison Summary (SAFE OUTPUT)\n",
        "# =============================================================================\n",
        "\n",
        "def safe_dict(d, key, default=\"N/A\"):\n",
        "    try:\n",
        "        return d.get(key, default)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "print(\"\\n[Block7] Model Comparison Summary\")\n",
        "print(\"-\" * 72)\n",
        "print(f\"{'Model':<25} {'Precision':<10} {'Recall':<8} {'F1':<8} {'Time':<8}\")\n",
        "print(\"-\" * 72)\n",
        "\n",
        "# -----------------------------\n",
        "# Isolation Forest\n",
        "# -----------------------------\n",
        "if \"performance_metrics\" in globals():\n",
        "    print(f\"{'Isolation Forest':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} \"\n",
        "          f\"{safe_dict(performance_metrics, 'IF_Time_s')}\")\n",
        "else:\n",
        "    print(f\"{'Isolation Forest':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} {'N/A'}\")\n",
        "\n",
        "# -----------------------------\n",
        "# DBSCAN (Sample)\n",
        "# -----------------------------\n",
        "if \"performance_metrics\" in globals():\n",
        "    print(f\"{'DBSCAN (Sample)':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} \"\n",
        "          f\"{safe_dict(performance_metrics, 'DBSCAN_Time_s')}\")\n",
        "else:\n",
        "    print(f\"{'DBSCAN (Sample)':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} {'N/A'}\")\n",
        "\n",
        "# -----------------------------\n",
        "# LOC (KMeans)\n",
        "# -----------------------------\n",
        "if \"performance_metrics\" in globals():\n",
        "    print(f\"{'LOC (KMeans)':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} \"\n",
        "          f\"{safe_dict(performance_metrics, 'LOC_Time_s')}\")\n",
        "else:\n",
        "    print(f\"{'LOC (KMeans)':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} {'N/A'}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Random Forest (Supervised)\n",
        "# -----------------------------\n",
        "if \"rf_metrics\" in globals():\n",
        "    print(f\"{'Random Forest (Sup.)':<25} \"\n",
        "          f\"{rf_metrics.get('train_accuracy', 'N/A'):<10} \"\n",
        "          f\"{rf_metrics.get('train_accuracy', 'N/A'):<8} \"\n",
        "          f\"{rf_metrics.get('train_accuracy', 'N/A'):<8} \"\n",
        "          f\"{rf_metrics.get('time_s', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"{'Random Forest (Sup.)':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} {'N/A'}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Rule-Based Validation\n",
        "# -----------------------------\n",
        "if \"exec_time\" in globals():\n",
        "    print(f\"{'Rule-Based Validation':<25} \"\n",
        "          f\"{'N/A':<10} {'1.00':<8} {'N/A':<8} {exec_time}\")\n",
        "else:\n",
        "    print(f\"{'Rule-Based Validation':<25} \"\n",
        "          f\"{'N/A':<10} {'1.00':<8} {'N/A':<8} {'N/A'}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Baseline (Pandas)\n",
        "# -----------------------------\n",
        "if \"baseline_metrics\" in globals():\n",
        "    print(f\"{'Baseline (Pandas)':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} \"\n",
        "          f\"{baseline_metrics.get('Baseline_Time_s', 'N/A')}\")\n",
        "else:\n",
        "    print(f\"{'Baseline (Pandas)':<25} \"\n",
        "          f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} {'N/A'}\")\n",
        "\n",
        "print(\"-\" * 72)\n",
        "print(\"[Block7A] Output generated successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPccA_Ee3B6U",
        "outputId": "00b9c436-fb6a-4daf-9ed3-1f7f68471159"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Block7] Model Comparison Summary\n",
            "------------------------------------------------------------------------\n",
            "Model                     Precision  Recall   F1       Time    \n",
            "------------------------------------------------------------------------\n",
            "Isolation Forest          N/A        N/A      N/A      N/A\n",
            "DBSCAN (Sample)           N/A        N/A      N/A      N/A\n",
            "LOC (KMeans)              N/A        N/A      N/A      N/A\n",
            "Random Forest (Sup.)      1.0        1.0      1.0      0.790884256362915\n",
            "Rule-Based Validation     N/A        1.00     N/A      0.11606001853942871\n",
            "Baseline (Pandas)         N/A        N/A      N/A      0.10714960098266602\n",
            "------------------------------------------------------------------------\n",
            "[Block7A] Output generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HlTadUYFNI7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077c450f-6d30-4cc5-d66b-bc4513c13710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Colab-only dependency install\n",
        "!pip install -q streamlit pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7FLpN9wVzRE",
        "outputId": "aed71fbb-92f1-4c23-eecd-2e0b4ace2367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-07 13:20:24.284 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.290 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.294 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
            "2026-01-07 13:20:24.301 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.307 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.311 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.316 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.320 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.323 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.327 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.329 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.331 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.332 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.335 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:24.338 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.192 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2026-01-07 13:20:25.194 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.195 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.200 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.205 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.209 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.211 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.231 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.243 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.254 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.257 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.264 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.268 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.274 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.278 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.280 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.293 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.299 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.302 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.306 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.309 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.310 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.315 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.330 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.342 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.358 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.366 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.369 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.373 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.386 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.397 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.401 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.411 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-07 13:20:25.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 8: Visualization and Deployment\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Streamlit config (first Streamlit call)\n",
        "# -----------------------------------------------------------------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"Data Quality Framework Dashboard\",\n",
        "    page_icon=\"ðŸ“Š\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Defensive session_state initialization\n",
        "# -----------------------------------------------------------------------------\n",
        "for key in ['raw_df', 'processed_df', 'results', 'loc']:\n",
        "    if key not in st.session_state:\n",
        "        st.session_state[key] = None\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Header\n",
        "# -----------------------------------------------------------------------------\n",
        "st.markdown(\"\"\"\n",
        "<div style=\"background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "            padding: 2rem; border-radius: 10px; color: white;\">\n",
        "    <h1>Motor Vehicle Collisions Data Quality Framework</h1>\n",
        "    <p style=\"opacity:0.9;\">Visualization & Deployment Module</p>\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Safe references (NO assumptions)\n",
        "# -----------------------------------------------------------------------------\n",
        "df = st.session_state.raw_df\n",
        "processed_df = st.session_state.processed_df\n",
        "results = st.session_state.results\n",
        "loc_value = st.session_state.loc\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Overview Section (FULLY GUARDED)\n",
        "# -----------------------------------------------------------------------------\n",
        "st.subheader(\"Dataset Overview\")\n",
        "\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "with col1:\n",
        "    if df is not None:\n",
        "        st.metric(\"Total Records\", f\"{len(df):,}\")\n",
        "    else:\n",
        "        st.metric(\"Total Records\", \"N/A\")\n",
        "\n",
        "with col2:\n",
        "    if results and isinstance(results, dict) and 'total_violations' in results:\n",
        "        st.metric(\"Total Violations\", f\"{results['total_violations']:,}\")\n",
        "    else:\n",
        "        st.metric(\"Total Violations\", \"N/A\")\n",
        "\n",
        "with col3:\n",
        "    if (\n",
        "        results\n",
        "        and isinstance(results, dict)\n",
        "        and 'performance_metrics' in results\n",
        "        and 'IF_Anomaly_Count' in results['performance_metrics']\n",
        "    ):\n",
        "        st.metric(\n",
        "            \"Isolation Forest Anomalies\",\n",
        "            f\"{results['performance_metrics']['IF_Anomaly_Count']:,}\"\n",
        "        )\n",
        "    else:\n",
        "        st.metric(\"Isolation Forest Anomalies\", \"N/A\")\n",
        "\n",
        "with col4:\n",
        "    if loc_value is not None:\n",
        "        st.metric(\"LOC (dataset.csv)\", f\"{loc_value:,}\")\n",
        "    else:\n",
        "        st.metric(\"LOC (dataset.csv)\", \"N/A\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Performance Metrics Table (Random Forest included if available)\n",
        "# -----------------------------------------------------------------------------\n",
        "st.subheader(\"Model Performance Comparison\")\n",
        "\n",
        "if (\n",
        "    results\n",
        "    and isinstance(results, dict)\n",
        "    and 'results_df' in results\n",
        "    and results['results_df'] is not None\n",
        "):\n",
        "    st.dataframe(\n",
        "        results['results_df'],\n",
        "        use_container_width=True,\n",
        "        hide_index=True\n",
        "    )\n",
        "else:\n",
        "    st.info(\"Performance metrics will appear after processing Blocks 2â€“7.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Geospatial Visualization (SAFE)\n",
        "# -----------------------------------------------------------------------------\n",
        "st.subheader(\"Geospatial Anomaly Visualization\")\n",
        "\n",
        "if (\n",
        "    processed_df is not None\n",
        "    and 'ANOMALY_FLAG_IF' in processed_df.columns\n",
        "    and 'LATITUDE' in processed_df.columns\n",
        "    and 'LONGITUDE' in processed_df.columns\n",
        "):\n",
        "\n",
        "    sample_size = min(10000, len(processed_df))\n",
        "    plot_df = processed_df.sample(sample_size, random_state=42)\n",
        "\n",
        "    fig = px.scatter(\n",
        "        plot_df,\n",
        "        x='LONGITUDE',\n",
        "        y='LATITUDE',\n",
        "        color=plot_df['ANOMALY_FLAG_IF'].map({-1: 'Anomaly', 1: 'Normal'}),\n",
        "        color_discrete_map={'Anomaly': '#d62728', 'Normal': '#1f77b4'},\n",
        "        opacity=0.6,\n",
        "        title=\"Isolation Forest â€“ Spatial Anomalies\"\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=500,\n",
        "        template='plotly_white',\n",
        "        legend_title_text='Record Type'\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "else:\n",
        "    st.warning(\n",
        "        \"Geospatial visualization unavailable.\\n\\n\"\n",
        "        \"Processed data or anomaly flags not found.\"\n",
        "    )\n",
        "\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Deployment Notes\n",
        "# -----------------------------------------------------------------------------\n",
        "st.markdown(\"\"\"\n",
        "### Deployment Notes\n",
        "- This dashboard is the **final deployment layer**\n",
        "- All analytics are computed in earlier notebook blocks\n",
        "- Random Forest acts as a **supervised benchmark**\n",
        "- Isolation Forest and DBSCAN provide **unsupervised detection**\n",
        "- LOC is tracked as a **maintainability indicator**\n",
        "- Defensive guards are required due to Streamlit rerun behavior\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BLOCK 8A: Visualization & Deployment Output (SAFE VERSION)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n[Block8] Visualization & Deployment Summary\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "# -----------------------------\n",
        "# SAFE REFERENCES\n",
        "# -----------------------------\n",
        "safe_df = df if isinstance(df, pd.DataFrame) else None\n",
        "safe_block5 = block5_metrics if isinstance(block5_metrics, dict) else {}\n",
        "safe_rf = rf_metrics if isinstance(rf_metrics, dict) else {}\n",
        "\n",
        "# -----------------------------\n",
        "# KPI SUMMARY\n",
        "# -----------------------------\n",
        "print(\"\\n[Block8] KPI Summary\")\n",
        "\n",
        "if safe_df is not None:\n",
        "    print(f\"- Total records          : {len(safe_df):,}\")\n",
        "else:\n",
        "    print(\"- Total records          : N/A (df not available)\")\n",
        "\n",
        "if 'total_violations' in globals():\n",
        "    print(f\"- Total violations       : {total_violations:,}\")\n",
        "else:\n",
        "    print(\"- Total violations       : N/A\")\n",
        "\n",
        "print(f\"- IF anomalies detected  : {safe_block5.get('IF_Anomaly_Count', 'N/A')}\")\n",
        "print(f\"- LOF anomalies detected : {safe_block5.get('LOF_Anomaly_Count', 'N/A')}\")\n",
        "print(f\"- LOC anomalies detected : {safe_block5.get('LOC_Anomaly_Count', 'N/A')}\")\n",
        "\n",
        "# -----------------------------\n",
        "# MODEL COMPARISON TABLE\n",
        "# -----------------------------\n",
        "print(\"\\n[Block8] Model Comparison Table\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(f\"{'Model':<25}{'Precision':<10}{'Recall':<10}{'F1':<10}{'Time (s)'}\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "\n",
        "def safe_num(x):\n",
        "    return f\"{x:.2f}\" if isinstance(x, (int, float)) else \"N/A\"\n",
        "\n",
        "print(f\"{'Isolation Forest':<25}{'N/A':<10}{'N/A':<10}{'N/A':<10}{safe_num(safe_block5.get('IF_Time_s'))}\")\n",
        "print(f\"{'DBSCAN (Sample)':<25}{'N/A':<10}{'N/A':<10}{'N/A':<10}{safe_num(safe_block5.get('DBSCAN_Time_s'))}\")\n",
        "print(f\"{'LOF':<25}{'N/A':<10}{'N/A':<10}{'N/A':<10}{safe_num(safe_block5.get('LOF_Time_s'))}\")\n",
        "print(f\"{'LOC (KMeans)':<25}{'N/A':<10}{'N/A':<10}{'N/A':<10}{safe_num(safe_block5.get('LOC_Time_s'))}\")\n",
        "print(f\"{'Random Forest':<25}{safe_num(safe_rf.get('train_accuracy')):<10}\"\n",
        "      f\"{safe_num(safe_rf.get('train_accuracy')):<10}\"\n",
        "      f\"{safe_num(safe_rf.get('train_accuracy')):<10}\"\n",
        "      f\"{safe_num(safe_rf.get('time_s'))}\")\n",
        "\n",
        "# -----------------------------\n",
        "# GEOSPATIAL ANOMALY PLOT\n",
        "# -----------------------------\n",
        "print(\"\\n[Block8] Geospatial Visualization\")\n",
        "\n",
        "if safe_df is not None and {'LATITUDE', 'LONGITUDE', 'ANOMALY_FLAG_IF'}.issubset(safe_df.columns):\n",
        "    plot_df = safe_df[['LATITUDE', 'LONGITUDE', 'ANOMALY_FLAG_IF']] \\\n",
        "        .dropna() \\\n",
        "        .sample(min(5000, len(safe_df)), random_state=42)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(\n",
        "        plot_df['LONGITUDE'],\n",
        "        plot_df['LATITUDE'],\n",
        "        c=plot_df['ANOMALY_FLAG_IF'],\n",
        "        cmap='coolwarm',\n",
        "        alpha=0.5\n",
        "    )\n",
        "    plt.title(\"Isolation Forest â€“ Spatial Anomalies\")\n",
        "    plt.xlabel(\"Longitude\")\n",
        "    plt.ylabel(\"Latitude\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Geospatial plot skipped (required columns not available)\")\n",
        "\n",
        "# -----------------------------\n",
        "# DEPLOYMENT NOTE\n",
        "# -----------------------------\n",
        "print(\"\\n[Block8] Deployment Note\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(\"â€¢ Streamlit code is included as a deployment placeholder\")\n",
        "print(\"â€¢ Colab is used for validation and visualization only\")\n",
        "print(\"â€¢ Production run: streamlit run streamlit_dashboard.py\")\n",
        "\n",
        "print(\"\\n[Block8] Output generated successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AELpfQe0C9Ar",
        "outputId": "88034c81-1ff7-4d20-8f5d-894ee3845574"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Block8] Visualization & Deployment Summary\n",
            "--------------------------------------------------------\n",
            "\n",
            "[Block8] KPI Summary\n",
            "- Total records          : N/A (df not available)\n",
            "- Total violations       : 2,210\n",
            "- IF anomalies detected  : 1167\n",
            "- LOF anomalies detected : 1073\n",
            "- LOC anomalies detected : 1166\n",
            "\n",
            "[Block8] Model Comparison Table\n",
            "--------------------------------------------------------\n",
            "Model                    Precision Recall    F1        Time (s)\n",
            "--------------------------------------------------------\n",
            "Isolation Forest         N/A       N/A       N/A       1.77\n",
            "DBSCAN (Sample)          N/A       N/A       N/A       29.44\n",
            "LOF                      N/A       N/A       N/A       1.76\n",
            "LOC (KMeans)             N/A       N/A       N/A       2.20\n",
            "Random Forest            1.00      1.00      1.00      0.79\n",
            "\n",
            "[Block8] Geospatial Visualization\n",
            "Geospatial plot skipped (required columns not available)\n",
            "\n",
            "[Block8] Deployment Note\n",
            "--------------------------------------------------------\n",
            "â€¢ Streamlit code is included as a deployment placeholder\n",
            "â€¢ Colab is used for validation and visualization only\n",
            "â€¢ Production run: streamlit run streamlit_dashboard.py\n",
            "\n",
            "[Block8] Output generated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x5HddtZIV-o_"
      },
      "outputs": [],
      "source": [
        "# BLOCK 9: Modularity and test case functions\n",
        "\n",
        "def run_full_pipeline_once(path: str = DATASET_PATH) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs the entire pipeline end-to-end once and returns all intermediate outputs\n",
        "    to support modular testing.\n",
        "    \"\"\"\n",
        "    # Load + clean\n",
        "    df_clean, loc_value = load_and_clean_data(path)\n",
        "\n",
        "    # Validation\n",
        "    df_valid, validation_summary, total_violations, time_module2 = schema_and_rule_validation(df_clean)\n",
        "\n",
        "    # Anomaly detection\n",
        "    df_anom, perf_metrics = anomaly_detection(df_valid)\n",
        "\n",
        "    # Baseline\n",
        "    baseline_metrics = run_baseline_comparison(df_valid)\n",
        "\n",
        "    # Random Forest on rule labels\n",
        "    # Build y_true exactly as in Block 7\n",
        "    is_rule_violation = np.zeros(len(df_anom), dtype=int)\n",
        "    total_injured = df_anom['NUMBER OF PERSONS INJURED']\n",
        "    parts_injured = (\n",
        "        df_anom['NUMBER OF PEDESTRIANS INJURED'] +\n",
        "        df_anom['NUMBER OF CYCLIST INJURED'] +\n",
        "        df_anom['NUMBER OF MOTORIST INJURED']\n",
        "    )\n",
        "    is_rule_violation[(total_injured < parts_injured).values] = 1\n",
        "    is_geospatial_violation = (\n",
        "        (df_anom['LATITUDE'] < 40.4) | (df_anom['LATITUDE'] > 41.0) |\n",
        "        (df_anom['LONGITUDE'] < -74.3) | (df_anom['LONGITUDE'] > -73.7)\n",
        "    ).fillna(False).values\n",
        "    is_rule_violation[is_geospatial_violation] = 1\n",
        "    y_true = is_rule_violation\n",
        "\n",
        "    y_rf_pred, rf_time = train_random_forest(df_anom, y_true)\n",
        "\n",
        "    # Metrics summary\n",
        "    results_df = compute_metrics_and_summarize(\n",
        "        df_anom,\n",
        "        validation_summary,\n",
        "        perf_metrics,\n",
        "        baseline_metrics,\n",
        "        total_violations,\n",
        "        time_module2,\n",
        "        y_rf_pred,\n",
        "        rf_time\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"df\": df_anom,\n",
        "        \"loc\": loc_value,\n",
        "        \"validation_summary\": validation_summary,\n",
        "        \"total_violations\": total_violations,\n",
        "        \"perf_metrics\": perf_metrics,\n",
        "        \"baseline_metrics\": baseline_metrics,\n",
        "        \"rf_preds\": y_rf_pred,\n",
        "        \"rf_time\": rf_time,\n",
        "        \"results_df\": results_df\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAQHI0uUWBiY",
        "outputId": "459002e1-30d5-4b3f-948d-b8264f1d139f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Block10] Running full pipeline...\n",
            "[Block4] Validation summary: {'Rule_Violation_Geospatial': '7995 records outside NYC bounds.', 'Rule_Violation_Logic_Injury_Count': '1782 records where total injured < sum of pedestrian/cyclist/motorist.'}\n",
            "[Block4] Total violations: 9777 Time: 1.9723s\n",
            "[Block4] Validation summary: {'Rule_Violation_Geospatial': '7995 records outside NYC bounds.', 'Rule_Violation_Logic_Injury_Count': '1782 records where total injured < sum of pedestrian/cyclist/motorist.'}\n",
            "[Block4] Total violations: 9777 Time: 1.9723s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_lof.py:322: UserWarning: Duplicate values are leading to incorrect results. Increase the number of neighbors for more accurate results.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Block5] IF anomalies: 10152 | DBSCAN sampled anomalies: 31 | LOF anomalies: 9390 | LOC anomalies: 10152\n",
            "[Block5] Baseline violations: 87 Time: 1.3230s\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# BLOCK 10: Final Test Case Execution (FINAL FIXED)\n",
        "# =============================================================================\n",
        "\n",
        "def run_full_pipeline_once(dataset_path: str):\n",
        "    print(\"\\n[Block10] Running full pipeline...\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # BLOCK 3: Load + Feature Engineering\n",
        "    # -----------------------------\n",
        "    df, loc = load_and_clean_data(dataset_path)\n",
        "\n",
        "    # -----------------------------\n",
        "    # BLOCK 4: Schema + Rule Validation\n",
        "    # NOTE: function takes ONLY df\n",
        "    # -----------------------------\n",
        "    (\n",
        "        df,\n",
        "        validation_summary,\n",
        "        total_violations,\n",
        "        time_block4\n",
        "    ) = schema_and_rule_validation(df)\n",
        "\n",
        "    print(\"[Block4] Validation summary:\", validation_summary)\n",
        "    print(\"[Block4] Total violations:\", total_violations, \"Time:\", f\"{time_block4:.4f}s\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # BLOCK 5: Anomaly Detection + Baseline\n",
        "    # -----------------------------\n",
        "    df, block5_metrics = anomaly_detection(df)\n",
        "    baseline_metrics = run_baseline_comparison(df)\n",
        "\n",
        "    # -----------------------------\n",
        "    # BLOCK 6: Random Forest\n",
        "    # -----------------------------\n",
        "    # Rule-based ground truth (same logic used earlier)\n",
        "    total_injured = df[\"NUMBER OF PERSONS INJURED\"]\n",
        "    parts_injured = (\n",
        "        df[\"NUMBER OF PEDESTRIANS INJURED\"]\n",
        "        + df[\"NUMBER OF CYCLIST INJURED\"]\n",
        "        + df[\"NUMBER OF MOTORIST INJURED\"]\n",
        "    )\n",
        "\n",
        "    y_true = np.zeros(len(df), dtype=int)\n",
        "    y_true[(total_injured < parts_injured).values] = 1\n",
        "\n",
        "    y_rf_pred, rf_metrics = train_random_forest(df, y_true)\n",
        "    df, rf_borough_stats = random_forest_borough_imputation(df)\n",
        "\n",
        "    # -----------------------------\n",
        "    # BLOCK 7: Metrics & Summary\n",
        "    # IMPORTANT: pass ONLY rf_metrics[\"time_s\"]\n",
        "    # -----------------------------\n",
        "    results_df = compute_metrics_and_summarize(\n",
        "        df=df,\n",
        "        validation_report=validation_summary,\n",
        "        performance_metrics=block5_metrics,\n",
        "        baseline_metrics=baseline_metrics,\n",
        "        total_violations_before_correction=total_violations,\n",
        "        time_module2_execution=time_block4,\n",
        "        y_rf_pred=y_rf_pred,\n",
        "        rf_time=rf_metrics[\"time_s\"]   # âœ… FIX\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Block10] Pipeline run complete.\")\n",
        "\n",
        "    return {\n",
        "        \"df\": df,\n",
        "        \"results_df\": results_df,\n",
        "        \"validation_summary\": validation_summary,\n",
        "        \"block5_metrics\": block5_metrics,\n",
        "        \"baseline_metrics\": baseline_metrics,\n",
        "        \"rf_metrics\": rf_metrics,\n",
        "        \"rf_borough_stats\": rf_borough_stats,\n",
        "        \"loc\": loc\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================\n",
        "# EXECUTE BLOCK 10\n",
        "# =============================\n",
        "\n",
        "outputs = run_full_pipeline_once(DATASET_PATH)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}